{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "X4jWesX_BBkN",
        "dna3-dPtc2T4",
        "orrqr5RtBTS8",
        "eGfMLPPDdlru",
        "iyIn2j1lducS",
        "8lABL4wbd0TB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Setup and Imports**"
      ],
      "metadata": {
        "id": "X4jWesX_BBkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxaoLrIWTcu4",
        "outputId": "2a85bf68-905e-4dc0-bed0-ca9513dd7b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Breast-Cancer-Prognosis-Prediction'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 44 (delta 12), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (44/44), 795.10 KiB | 2.42 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "/content/Breast-Cancer-Prognosis-Prediction/Breast-Cancer-Prognosis-Prediction\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np, pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from tensorflow.keras.layers import Concatenate, BatchNormalization, Input, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "BMZPhb6TyB_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Data Loading and Header Processing*"
      ],
      "metadata": {
        "id": "dna3-dPtc2T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to merge multi-index headers to single index headers\n",
        "def merge_headers(col_tuple):\n",
        "    # Unpack the tuple: first-level and second-level names\n",
        "    first, second = col_tuple\n",
        "\n",
        "    # If newline characters exist, remove and replace with space\n",
        "    if isinstance(first, str):\n",
        "        first = first.replace('\\n', ' ').strip()\n",
        "    if isinstance(second, str):\n",
        "        second = second.replace('\\n', ' ').strip()\n",
        "\n",
        "    # If blank second-headers exist, return first-header only\n",
        "    if not second or 'Unnamed' in second:\n",
        "        return first\n",
        "    # Otherwise, return merged header\n",
        "    else:\n",
        "        return f\"{first} - {second}\""
      ],
      "metadata": {
        "id": "Zc2fAw6J0qQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Data Encoding*\n"
      ],
      "metadata": {
        "id": "orrqr5RtBTS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_clinical_data(df):\n",
        "    \"\"\"\n",
        "    Encodes clinical data with the understanding that real data starts at row 4.\n",
        "    Rows 1-3 contain header/metadata information.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        The clinical dataframe to encode\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        The encoded dataframe with all columns properly processed\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original\n",
        "    encoded_df = df.copy()\n",
        "\n",
        "    # pull off patientIDs\n",
        "    patient_ids = encoded_df['Patient ID'].copy().reset_index(drop=True)\n",
        "    encoded_df = encoded_df.drop(columns=['Patient ID'])\n",
        "\n",
        "    # First, check if we need to handle the header rows\n",
        "    # If the dataframe has already been loaded with headers processed\n",
        "    # (i.e., headers are in column names), we don't need this step\n",
        "    if encoded_df.shape[0] >= 4:\n",
        "        print(\"Checking if data starts at row 4...\")\n",
        "        # Sample some values to see if first 3 rows appear to be headers\n",
        "        sample_col = encoded_df.columns[0]\n",
        "        first_rows = encoded_df.loc[0:3, sample_col].tolist()\n",
        "        print(f\"First rows of sample column: {first_rows}\")\n",
        "\n",
        "        # If first rows look like headers, remove them\n",
        "        if any(isinstance(val, str) and '=' in str(val) for val in first_rows):\n",
        "            print(\"First rows appear to contain metadata. Removing rows 0-3...\")\n",
        "            encoded_df = encoded_df.iloc[3:].reset_index(drop=True)\n",
        "            print(f\"Dataframe shape after removing header rows: {encoded_df.shape}\")\n",
        "\n",
        "    # Identify the target column\n",
        "    target_col = None\n",
        "    for col in encoded_df.columns:\n",
        "        if \"Recurrence event\" in col:\n",
        "            target_col = col\n",
        "            target_values = encoded_df[target_col].copy()\n",
        "            print(f\"Identified target column: {target_col}\")\n",
        "            break\n",
        "\n",
        "    # Process each column individually\n",
        "    all_columns = encoded_df.columns.tolist()\n",
        "    print(f\"Processing {len(all_columns)} total columns\")\n",
        "\n",
        "    for col in all_columns:\n",
        "        # Skip target column for now\n",
        "        if col == target_col:\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing column: {col}\")\n",
        "\n",
        "        try:\n",
        "            # Check column data type\n",
        "            if encoded_df[col].dtype == 'object':\n",
        "                # Categorical column\n",
        "                print(f\"  Processing as categorical\")\n",
        "\n",
        "                # Fill missing values\n",
        "                encoded_df[col] = encoded_df[col].fillna(\"MISSING\")\n",
        "\n",
        "                # Convert to string\n",
        "                encoded_df[col] = encoded_df[col].astype(str)\n",
        "\n",
        "                # Apply label encoding\n",
        "                le = LabelEncoder()\n",
        "                encoded_df[col] = le.fit_transform(encoded_df[col])\n",
        "                print(f\"  Encoded {len(le.classes_)} unique values\")\n",
        "\n",
        "            else:\n",
        "                # Numeric column\n",
        "                print(f\"  Processing as numeric\")\n",
        "\n",
        "                # Handle missing values\n",
        "                if encoded_df[col].isna().any():\n",
        "                    if encoded_df[col].isna().all():\n",
        "                        encoded_df[col] = 0\n",
        "                        print(f\"  All values missing, filled with 0\")\n",
        "                    else:\n",
        "                        median = encoded_df[col].median()\n",
        "                        encoded_df[col] = encoded_df[col].fillna(median)\n",
        "                        print(f\"  Filled missing values with median: {median}\")\n",
        "\n",
        "                # Standardize if there's variance\n",
        "                if encoded_df[col].std() > 0:\n",
        "                    mean_val = encoded_df[col].mean()\n",
        "                    std_val = encoded_df[col].std()\n",
        "                    encoded_df[col] = (encoded_df[col] - mean_val) / std_val\n",
        "                    print(f\"  Standardized numeric column\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing column {col}: {str(e)}\")\n",
        "\n",
        "            # Try alternative approach\n",
        "            try:\n",
        "                print(f\"  Trying alternative encoding approach\")\n",
        "\n",
        "                # Force to string and encode\n",
        "                encoded_df[col] = encoded_df[col].fillna(\"MISSING\")\n",
        "                encoded_df[col] = encoded_df[col].astype(str)\n",
        "                le = LabelEncoder()\n",
        "                encoded_df[col] = le.fit_transform(encoded_df[col])\n",
        "                print(f\"  Alternative encoding successful\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(f\"  Alternative approach failed: {str(e2)}\")\n",
        "                print(f\"  Setting column to 0\")\n",
        "                encoded_df[col] = 0\n",
        "\n",
        "    # Restore target column\n",
        "    if target_col and 'target_values' in locals():\n",
        "        encoded_df[target_col] = target_values\n",
        "        print(f\"Restored target column: {target_col}\")\n",
        "\n",
        "    # Final check for any NaN values\n",
        "    if encoded_df.isna().any().any():\n",
        "        nan_cols = encoded_df.columns[encoded_df.isna().any()].tolist()\n",
        "        print(f\"Filling NaN values in {len(nan_cols)} columns\")\n",
        "        encoded_df = encoded_df.fillna(0)\n",
        "\n",
        "    # reattach patientIDs\n",
        "    encoded_df['Patient ID'] = patient_ids\n",
        "\n",
        "    print(f\"Final encoded dataframe shape: {encoded_df.shape}\")\n",
        "    return encoded_df"
      ],
      "metadata": {
        "id": "7BiYt8Pt4ubu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN Model**"
      ],
      "metadata": {
        "id": "4kXymPIuyRHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Model Building*"
      ],
      "metadata": {
        "id": "iyIn2j1lducS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_advanced_rnn_model(input_shape, rnn_type='LSTM', units=64,\n",
        "                            bidirectional=True, attention=False,\n",
        "                            dropout_rate=0.3, l1_reg=0.0001, l2_reg=0.0001):\n",
        "    \"\"\"\n",
        "    Build an advanced RNN model with various architectural improvements:\n",
        "    - Bidirectional RNN layers\n",
        "    - Batch normalization\n",
        "    - Regularization (dropout, L1, L2)\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_shape : tuple\n",
        "        Shape of input data (time_steps, features)\n",
        "    rnn_type : str, default='LSTM'\n",
        "        Type of RNN layer ('LSTM' or 'GRU')\n",
        "    units : int, default=64\n",
        "        Number of RNN units\n",
        "    bidirectional : bool, default=True\n",
        "        Whether to use bidirectional RNNs\n",
        "    attention : bool, default=False\n",
        "        Whether to add an attention mechanism (simplified)\n",
        "    dropout_rate : float, default=0.3\n",
        "        Dropout rate for regularization\n",
        "    l1_reg : float, default=0.0001\n",
        "        L1 regularization strength\n",
        "    l2_reg : float, default=0.0001\n",
        "        L2 regularization strength\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    keras.Model\n",
        "        Compiled RNN model\n",
        "    \"\"\"\n",
        "    # Simple version for simpler architectural choices\n",
        "    if not bidirectional:\n",
        "        model = Sequential()\n",
        "\n",
        "        # Use specified RNN type\n",
        "        if rnn_type == 'LSTM':\n",
        "            model.add(LSTM(units, input_shape=input_shape,\n",
        "                          kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                          recurrent_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                          return_sequences=False))\n",
        "        elif rnn_type == 'GRU':\n",
        "            model.add(GRU(units, input_shape=input_shape,\n",
        "                         kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                         recurrent_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                         return_sequences=False))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown RNN type: {rnn_type}\")\n",
        "\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # More complex model with bidirectional RNNs\n",
        "    else:\n",
        "        # Create functional API model for more flexibility\n",
        "        inputs = Input(shape=input_shape)\n",
        "\n",
        "        # Configure RNN layer based on parameters\n",
        "        if rnn_type == 'LSTM':\n",
        "            rnn_layer = LSTM(units,\n",
        "                            kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                            recurrent_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                            return_sequences=False)  # No need for sequences in this implementation\n",
        "        elif rnn_type == 'GRU':\n",
        "            rnn_layer = GRU(units,\n",
        "                           kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                           recurrent_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                           return_sequences=False)  # No need for sequences in this implementation\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown RNN type: {rnn_type}\")\n",
        "\n",
        "        # Add bidirectional wrapper\n",
        "        rnn_output = Bidirectional(rnn_layer)(inputs)\n",
        "\n",
        "        # Add dense layers with regularization\n",
        "        x = BatchNormalization()(rnn_output)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        x = Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "1tS-F_e110-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Model Training*\n"
      ],
      "metadata": {
        "id": "-lXUsuB3eaf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_advanced_callbacks(model, X_train, y_train, X_val, y_val,\n",
        "                                 batch_size=32, epochs=100,\n",
        "                                 early_stopping_patience=10,\n",
        "                                 reduce_lr_patience=5,\n",
        "                                 model_checkpoint_path='best_model.h5'):\n",
        "    \"\"\"\n",
        "    Train model with advanced callbacks for better performance\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : keras.Model\n",
        "        The compiled model to train\n",
        "    X_train, y_train : array-like\n",
        "        Training data and labels\n",
        "    X_val, y_val : array-like\n",
        "        Validation data and labels\n",
        "    batch_size : int, default=32\n",
        "        Batch size for training\n",
        "    epochs : int, default=100\n",
        "        Maximum number of epochs\n",
        "    early_stopping_patience : int, default=10\n",
        "        Patience for early stopping\n",
        "    reduce_lr_patience : int, default=5\n",
        "        Patience for learning rate reduction\n",
        "    model_checkpoint_path : str, default='best_model.h5'\n",
        "        Path to save the best model weights\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    history : dict\n",
        "        Training history\n",
        "    \"\"\"\n",
        "    # Define callbacks\n",
        "    callbacks = [\n",
        "        # Early stopping to prevent overfitting\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=early_stopping_patience,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        # Reduce learning rate when validation loss plateaus\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=reduce_lr_patience,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "        # Save the best model based on validation loss\n",
        "        ModelCheckpoint(\n",
        "            filepath=model_checkpoint_path,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "RHKFuH1neeUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Model Evaluation*"
      ],
      "metadata": {
        "id": "8lABL4wbd0TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_binary_classifier(model, X_test, y_test, class_names=['No Recurrence', 'Recurrence']):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of binary classifier with various metrics and plots\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : keras.Model\n",
        "        Trained model to evaluate\n",
        "    X_test, y_test : array-like\n",
        "        Test data and labels\n",
        "    class_names : list, default=['No Recurrence', 'Recurrence']\n",
        "        Names of the classes for plotting\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    # Get predictions\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_loss, test_accuracy, test_auc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "\n",
        "    # Add text annotations to the confusion matrix\n",
        "    thresh = cm.max() / 2\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                    horizontalalignment=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix_advanced.png')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('roc_curve_advanced.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.savefig('pr_curve_advanced.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Return all metrics in a dictionary\n",
        "    metrics = {\n",
        "        'accuracy': test_accuracy,\n",
        "        'auc': test_auc,\n",
        "        'precision': test_precision,\n",
        "        'recall': test_recall,\n",
        "        'f1_score': report['weighted avg']['f1-score'],\n",
        "        'confusion_matrix': cm,\n",
        "        'roc_auc': roc_auc,\n",
        "        'pr_auc': pr_auc\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Dy0b9c8g1veQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scripts"
      ],
      "metadata": {
        "id": "ynIBi-pRvi0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  # Clone repository and set working directory\n",
        "  !git clone https://github.com/alexander-harmaty/Breast-Cancer-Prognosis-Prediction.git\n",
        "  %cd Breast-Cancer-Prognosis-Prediction\n",
        "\n",
        "  # load the dataset from the repo root\n",
        "  file_path = './Clinical_and_Other_Features.xlsx'\n",
        "  clinical_df = pd.read_excel(file_path, header=[1,2])\n",
        "\n",
        "  # Load the data\n",
        "  file_path = './Clinical_and_Other_Features.xlsx'\n",
        "  clinical_df = pd.read_excel(file_path, header=[1, 2])\n",
        "\n",
        "  # Preprocess the column headers\n",
        "  # Merge multi-index headers for all columns\n",
        "  new_columns = [merge_headers(col) for col in clinical_df.columns]\n",
        "  clinical_df.columns = new_columns\n",
        "\n",
        "  # Print column info\n",
        "  print(f\"Total columns: {len(clinical_df.columns)}\")\n",
        "  print(f\"Sample size: {len(clinical_df)}\")\n",
        "\n",
        "  # target variable\n",
        "  target_col = \"Recurrence event(s) - {0 = no, 1 = yes}\"\n",
        "  if target_col not in clinical_df.columns:\n",
        "      # Find the correct column name by looking for a substring match\n",
        "      matching_cols = [col for col in clinical_df.columns if \"Recurrence event\" in col]\n",
        "      if matching_cols:\n",
        "          target_col = matching_cols[0]\n",
        "          print(f\"Found target column: {target_col}\")\n",
        "      else:\n",
        "          raise ValueError(\"Target column not found! Please check the column names.\")\n",
        "\n",
        "  # Encode the data\n",
        "  encoded_df = encode_clinical_data(clinical_df)\n",
        "  print(f\"Encoded data shape: {encoded_df.shape}\")\n",
        "\n",
        "  # Split the data into features and target\n",
        "  X = encoded_df.drop(columns=[target_col, 'Patient ID'])\n",
        "  y = encoded_df[target_col]\n",
        "  # X = encoded_df.drop(columns=[target_col]) if target_col in encoded_df.columns else encoded_df\n",
        "  # y = encoded_df[target_col] if target_col in encoded_df.columns else None\n",
        "\n",
        "\n",
        "  # Print info about target distribution\n",
        "  if y is not None:\n",
        "      print(f\"Target distribution:\\n{y.value_counts()}\")\n",
        "  else:\n",
        "      print(\"Warning: Target column not found in encoded dataframe!\")\n",
        "\n",
        "  # Split data into train, validation, and test sets\n",
        "  X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "  print(\"Training set size:\", X_train.shape, y_train.shape)\n",
        "  print(\"Validation set size:\", X_val.shape, y_val.shape)\n",
        "  print(\"Test set size:\", X_test.shape, y_test.shape)\n",
        "\n",
        "  # Reshape data for RNN (sequence data)\n",
        "  # RNNs expect input of shape (batch_size, time_steps, features)\n",
        "  X_train_seq = np.expand_dims(X_train.values.astype('float32'), axis=1)\n",
        "  X_val_seq   = np.expand_dims(X_val.values.astype('float32'),   axis=1)\n",
        "  X_test_seq  = np.expand_dims(X_test.values.astype('float32'),  axis=1)\n",
        "  # X_train_seq = np.expand_dims(X_train.values, axis=1)  # shape: (samples, 1, features)\n",
        "  # X_val_seq = np.expand_dims(X_val.values, axis=1)\n",
        "  # X_test_seq = np.expand_dims(X_test.values, axis=1)\n",
        "\n",
        "\n",
        "  # Check for NaN values using np.isnan for NumPy arrays\n",
        "  if np.isnan(X_test_seq).any():\n",
        "      print(\"Warning: NaN values found in test data! Filling with 0...\")\n",
        "      X_test_seq = np.nan_to_num(X_test_seq, nan=0.0)\n",
        "\n",
        "  X_train_seq = np.nan_to_num(X_train_seq, nan=0.0)\n",
        "  X_val_seq = np.nan_to_num(X_val_seq, nan=0.0)\n",
        "\n",
        "  print(\"Sequence shapes:\")\n",
        "  print(\"X_train_seq:\", X_train_seq.shape)\n",
        "  print(\"X_val_seq:\", X_val_seq.shape)\n",
        "  print(\"X_test_seq:\", X_test_seq.shape)\n",
        "\n",
        "  # Build the advanced RNN model\n",
        "  input_shape = (X_train_seq.shape[1], X_train_seq.shape[2])  # (time_steps, features)\n",
        "  advanced_model = build_advanced_rnn_model(\n",
        "      input_shape=input_shape,\n",
        "      rnn_type='LSTM',       # 'LSTM' or 'GRU'\n",
        "      units=128,             # Number of RNN units\n",
        "      bidirectional=True,    # Use bidirectional RNN\n",
        "      attention=False,       # Attention mechanism not needed for this data\n",
        "      dropout_rate=0.3,      # Dropout rate for regularization\n",
        "      l1_reg=0.0001,         # L1 regularization strength\n",
        "      l2_reg=0.0001          # L2 regularization strength\n",
        "  )\n",
        "\n",
        "  # Train the model with advanced callbacks\n",
        "  history = train_with_advanced_callbacks(\n",
        "      model=advanced_model,\n",
        "      X_train=X_train_seq,\n",
        "      y_train=y_train,\n",
        "      X_val=X_val_seq,\n",
        "      y_val=y_val,\n",
        "      batch_size=32,\n",
        "      epochs=100,\n",
        "      early_stopping_patience=10,\n",
        "      reduce_lr_patience=5,\n",
        "      model_checkpoint_path='best_rnn_model.h5'\n",
        "  )\n",
        "\n",
        "  # Evaluate the model\n",
        "  metrics = evaluate_binary_classifier(\n",
        "      model=advanced_model,\n",
        "      X_test=X_test_seq,\n",
        "      y_test=y_test\n",
        "  )\n",
        "\n",
        "  print(f\"Final model performance:\")\n",
        "  print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "  print(f\"AUC: {metrics['auc']:.4f}\")\n",
        "  print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "  from google.colab import files\n",
        "\n",
        "  # Build pickle file\n",
        "  best_rnn = load_model('best_rnn_model.h5')\n",
        "  penult = best_rnn.layers[-3].output\n",
        "  feat_extractor = Model(inputs=best_rnn.input, outputs=penult)\n",
        "  rnn_feats = feat_extractor.predict(np.vstack((X_train_seq, X_val_seq, X_test_seq)))\n",
        "  all_labels = np.concatenate([y_train, y_val, y_test])\n",
        "  rnn_ids = encoded_df['Patient ID'].tolist()\n",
        "\n",
        "  with open('rnn_features.pkl', 'wb') as f:\n",
        "      pickle.dump({\n",
        "        'features': rnn_feats,\n",
        "        'labels':   all_labels,\n",
        "        'ids':      rnn_ids,\n",
        "      }, f)\n",
        "\n",
        "  print(\"Saved RNN features to rnn_features.pkl\")\n",
        "\n",
        "  files.download('rnn_features.pkl')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "82H_o3ClwfXr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}